{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import join_tokens, tokenize_text\n",
    "\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "CHUNKSIZE = 256\n",
    "_ptrain= .7\n",
    "_pval = .15\n",
    "_ptest = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"anon_data/ner_data_2.json\",'r') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124089"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verfahrensbeteiligte Karin Marti, Beschwerdeführer, gegen Familiengericht Muri, Seetalstrasse 8, 5630 Muri. Gegenstand Erweiterung einer Beistandschaft, Beschwerde gegen den Entscheid des Obergerichts des Kantons Aargau, Kammer für Kindes - und Erwachsenenschutz, vom 23. Juni 2022 ( XBE. 2022. 16 ). Sachverhalt: Die Vorgeschichte ist dem Bundesgericht aus einer Vielzahl von Verfahren bekannt. Der Beschwerdeführer leidet an gutachterlich festgestellten Wahnvorstellungen und befindet sich zur Zeit in Untersuchungshaft. Seit dem 2. Juni 2021 besteht eine Vertretungsbeistandschaft mit Vermögensverwaltung mit teilweisem Entzug der Handlungsfähigkeit. Am 29. Oktober 2021 wandte er sich an das Familiengericht Muri und beantragte die Sistierung der Wohnungsmiete für die nächsten sechs Monate, damit er im Notfall seine Ex-Frau finanziell unterstützen könne. Mit Eingabe vom 3. November 2021 beantragte die Beiständin eine Ausweitung der Beistandschaft. Mit Entscheid vom 16. Februar 2022 weitete das Familiengericht die bestehende Beistandschaft auf verschiedene weitere Bereiche aus. Die hiergegen erhobene Beschwerde wies das Obergericht des Kantons Aargau mit Entscheid vom 23. Juni 2022 ab, soweit es darauf eintrat. Mit Beschwerde vom 13. Juli 2022 wendet sich der Beschwerdeführer an das Bundesgericht und stellt zahlreiche Strafanträge. Erwägungen: 1. Das Obergericht erwog, dass ohne Ausdehnung der Beistandschaft namentlich auf das gesamte Einkommen und Vermögen der Beschwerdeführer u. a. seine Wohnung verlieren könnte. In Bezug auf die geschuldeten Unterhaltszahlungen sende er widersprüchliche Signale aus; teils wolle er Zahlungen an seine Ex-Frau und die Kinder erbringen, dann wiederum nicht; die Ex-Frau sei seit der Eingabe vom 10. Dezember 2021 nun ebenfalls Teil seines Wahnsystems geworden, indem er sie der Beihilfe zum Mord an seiner Person verdächtige und diesbezüglich Strafantrag gestellt habe. Es sei davon auszugehen, dass der auch im neusten Gutachten festgestellte Wahn sich weiter ausgeweitet habe, was dazu führe, dass seine Handlungen objektiv nicht mehr nachvollziehbar seien, während sie subjektiv für ihn einen logischen Schluss seiner Wahnüberlegungen darstellten. Insbesondere sei er nicht mehr in der Lage nachzuvollziehen, dass er sich mit der Nichtbezahlung seiner Verbindlichkeiten selber schade. 2. Neue Begehren sind vor Bundesgericht unzulässig ( Art. 99 Abs. 2 BGG ) und soweit mehr oder anderes verlangt wird, als von der Vorinstanz beurteilt wurde, kann auf die Beschwerde von vornherein nicht eingetreten werden ( BGE 136 II 457 E. 4. 2; 136 V 362 E. 3. 4. 2; 142 I 155 E. 4. 4. 2 ). Die Beschwerde hat eine Begründung zu enthalten, in welcher in gedrängter Form dargelegt wird, inwiefern der angefochtene Entscheid Recht verletzt ( Art. 42 Abs. 2 BGG ), was eine sachbezogene Auseinandersetzung mit dessen Erwägungen erfordert ( BGE 140 III 115 E. 2; 142 III 364 E. 2. 4 ). 3. Weder stellt der Beschwerdeführer Begehren zur Sache noch setzt er sich in sachgerichteter Weise mit den Erwägungen des angefochtenen Entscheides auseinander. Vielmehr spricht er von einem betrügerischen Netzwerk, das aus dem Bundesgericht, der kantonalen Gerichtsaufsichtskommission, dem Obergericht, dem Verwaltungsgericht, dem Bezirksgericht, dem Familiengericht und dem Fachgericht bestehe und er stellt im Zusammenhang mit diversen angeblichen Straftaten Strafanträge gegen Richter aller Instanzen sowie gegen Staatsanwälte und weitere Personen ( Vollzugsbeamte, Familienmitglieder u. a. m. ). Ferner rügt er eine materielle Enteignung in Millionenumfang im Kontext mit der Sebastian Hochreutener - Gruppe, der Scheidung und Erbangelegenheiten. Ferner macht er geltend, alle psychiatrischen Gutachten angefochten zu haben, weshalb sie keine juristische Grundlage darstellen würden. All dies beschlägt nicht den Anfechtungsgegenstand. Ein irgendwie gearteter Hinweis auf mögliche Rechtsverletzungen durch den angefochtenen Entscheid ist nicht auszumachen. 4. Nach dem Gesagten erweist sich die Beschwerde als mehrheitlich offensichtlich unzulässig und im Übrigen als offensichtlich nicht hinreichend begründet, weshalb auf sie im vereinfachten Verfahren nach Art. 108 Abs. 1 lit. a und b BGG nicht einzutreten ist. 5. Die Gerichtskosten sind dem Beschwerdeführer aufzuerlegen ( Art. 66 Abs. 1 BGG ). Demnach erkennt das präsidierende Mitglied: 1. Auf die Beschwerde wird nicht eingetreten. 2. Die Gerichtskosten von Fr. 1 ' 000. - - werden dem Beschwerdeführer auferlegt. 3. Dieses Urteil wird dem Beschwerdeführer, dem Familiengericht Muri und dem Obergericht des Kantons Aargau, Kammer für Kindes - und Erwachsenenschutz, mitgeteilt. Lausanne\n",
      "[[1, 2, 'person'], [600, 601, 'person']]\n"
     ]
    }
   ],
   "source": [
    "print(join_tokens(data[110771]['tokenized_text']))\n",
    "print(data[110771]['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [data[i]['tokenized_text'] for i in range(len(data))]\n",
    "text_lens = [len(tokenized_texts[i]) for i in range(len(tokenized_texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 124089\n",
      "Mean text length: 2435.367913352513\n",
      "Max text length: 93790\n",
      "Min text length: 8\n",
      "Median text length: 1845\n",
      "Top 10 text lengths: [93790, 88627, 78435, 71139, 58708, 58119, 57739, 56936, 44997, 44226]\n"
     ]
    }
   ],
   "source": [
    "# print summary statistics about text lengths\n",
    "print('Number of texts:', len(tokenized_texts))\n",
    "print('Mean text length:', sum(text_lens) / len(tokenized_texts))\n",
    "print('Max text length:', max(text_lens))\n",
    "print('Min text length:', min(text_lens))\n",
    "print('Median text length:', sorted(text_lens)[len(text_lens) // 2])\n",
    "print('Top 10 text lengths:', sorted(text_lens, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 12408924089\n",
      "Number of chunks: 1242717\n"
     ]
    }
   ],
   "source": [
    "#chunk without offset into CHUNKSIZE, keep the document identity \n",
    "\n",
    "def chunk_data(sample, chunk_size):\n",
    "    tokenized_text = sample['tokenized_text']\n",
    "    ners = sample['ner']\n",
    "\n",
    "    tokenized_texts = []\n",
    "    start_token_indices = []\n",
    "    end_token_indices = []\n",
    "    labels = []\n",
    "    contains_ner = []\n",
    "    for i in range(0, len(tokenized_text), chunk_size):\n",
    "        start = i\n",
    "        end = i + chunk_size\n",
    "        tokenized_texts.append(tokenized_text[start:end])\n",
    "        new_starts = []\n",
    "        new_ends = []\n",
    "        new_labels = []\n",
    "        for ner_label in ners:\n",
    "            if ner_label[0] >= i and ner_label[0] < end and ner_label[1] < end: #make sure the whole ner range fits into the chunk\n",
    "                new_starts.append(ner_label[0]-i)\n",
    "                new_ends.append(ner_label[1] -i)\n",
    "                new_labels.append(ner_label[2])\n",
    "        contains_ner.append(len(new_labels) > 0) \n",
    "        start_token_indices.append(new_starts)\n",
    "        end_token_indices.append(new_ends) \n",
    "        labels.append(new_labels) \n",
    "\n",
    "    return tokenized_texts,start_token_indices,end_token_indices,labels,contains_ner\n",
    "\n",
    "def chunk_dataset_into_df(data : list[dict], chunk_size : int) -> pd.DataFrame:\n",
    "    document_ids = []\n",
    "    tokenized_texts = []\n",
    "    start_token_indices = []\n",
    "    end_token_indices = []\n",
    "    labels = []\n",
    "    contains_ner = []\n",
    "    for i, sample in enumerate(data):\n",
    "        print(f\"Processing sample {i + 1} of {len(data)}\", end='\\r')\n",
    "        new_tok_texts, new_starts, new_ends, new_labels, new_contains_ner = chunk_data(sample,chunk_size)\n",
    "        document_ids.extend([i]*(len(new_tok_texts)))\n",
    "        tokenized_texts.extend(new_tok_texts)\n",
    "        start_token_indices.extend(new_starts)\n",
    "        end_token_indices.extend(new_ends)\n",
    "        labels.extend(new_labels)\n",
    "        contains_ner.extend(new_contains_ner)\n",
    "\n",
    "    return pd.DataFrame({\"document_id\" : document_ids,\n",
    "                         \"tokenized_text\" : tokenized_texts,\n",
    "                         \"start_token_indices\" : start_token_indices,\n",
    "                         \"end_token_indices\" : end_token_indices,\n",
    "                         \"labels\" : labels,\n",
    "                         \"contains_ner\" : contains_ner\n",
    "                         })\n",
    "\n",
    "chunked_data = chunk_dataset_into_df(data, CHUNKSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 124089\n",
      "Number of chunks: 1242717\n"
     ]
    }
   ],
   "source": [
    "print('Number of data points:', len(data))\n",
    "print('Number of chunks:', len(chunked_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>start_token_indices</th>\n",
       "      <th>end_token_indices</th>\n",
       "      <th>labels</th>\n",
       "      <th>contains_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611381</th>\n",
       "      <td>67938</td>\n",
       "      <td>[Partecipanti, al, procedimento, Elisabeth, Ra...</td>\n",
       "      <td>[3, 134, 209, 83, 93, 89]</td>\n",
       "      <td>[4, 135, 210, 84, 94, 89]</td>\n",
       "      <td>[person, person, person, person, person, locat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155556</th>\n",
       "      <td>18721</td>\n",
       "      <td>[Oktober, 2006, ein, Panvertebralsyndrom, bei,...</td>\n",
       "      <td>[20, 168, 70]</td>\n",
       "      <td>[21, 168, 72]</td>\n",
       "      <td>[person, location, location]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222753</th>\n",
       "      <td>26330</td>\n",
       "      <td>[Jahren, nicht, mehr, tragbar, war, und, -, na...</td>\n",
       "      <td>[115]</td>\n",
       "      <td>[116]</td>\n",
       "      <td>[person]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99499</th>\n",
       "      <td>12384</td>\n",
       "      <td>[1015, ;, PETER, HÄNNI, ,, Planungs, -, Bau, -...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275143</th>\n",
       "      <td>32021</td>\n",
       "      <td>[Sinne, von, Art, ., 98, BGG, der, Beschwerde,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710148</th>\n",
       "      <td>77708</td>\n",
       "      <td>[père, et, la, mère, bénéficiant, d, ', un, li...</td>\n",
       "      <td>[215, 98]</td>\n",
       "      <td>[216, 99]</td>\n",
       "      <td>[person, person]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760664</th>\n",
       "      <td>82462</td>\n",
       "      <td>[., Aucun, fait, nouveau, ni, preuve, nouvelle...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24406</th>\n",
       "      <td>3515</td>\n",
       "      <td>[im, Sinne, von, Art, ., 16, MWSTV, nur, dann,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511792</th>\n",
       "      <td>57662</td>\n",
       "      <td>[von, weiteren, Eingaben, und, Verfahren, in, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940525</th>\n",
       "      <td>99083</td>\n",
       "      <td>[Verfahrensbeteiligte, Tonie, Hochstrasser, ,,...</td>\n",
       "      <td>[1, 68, 129]</td>\n",
       "      <td>[2, 69, 130]</td>\n",
       "      <td>[person, person, person]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        document_id                                     tokenized_text  \\\n",
       "611381        67938  [Partecipanti, al, procedimento, Elisabeth, Ra...   \n",
       "155556        18721  [Oktober, 2006, ein, Panvertebralsyndrom, bei,...   \n",
       "222753        26330  [Jahren, nicht, mehr, tragbar, war, und, -, na...   \n",
       "99499         12384  [1015, ;, PETER, HÄNNI, ,, Planungs, -, Bau, -...   \n",
       "275143        32021  [Sinne, von, Art, ., 98, BGG, der, Beschwerde,...   \n",
       "710148        77708  [père, et, la, mère, bénéficiant, d, ', un, li...   \n",
       "760664        82462  [., Aucun, fait, nouveau, ni, preuve, nouvelle...   \n",
       "24406          3515  [im, Sinne, von, Art, ., 16, MWSTV, nur, dann,...   \n",
       "511792        57662  [von, weiteren, Eingaben, und, Verfahren, in, ...   \n",
       "940525        99083  [Verfahrensbeteiligte, Tonie, Hochstrasser, ,,...   \n",
       "\n",
       "              start_token_indices          end_token_indices  \\\n",
       "611381  [3, 134, 209, 83, 93, 89]  [4, 135, 210, 84, 94, 89]   \n",
       "155556              [20, 168, 70]              [21, 168, 72]   \n",
       "222753                      [115]                      [116]   \n",
       "99499                          []                         []   \n",
       "275143                         []                         []   \n",
       "710148                  [215, 98]                  [216, 99]   \n",
       "760664                         []                         []   \n",
       "24406                          []                         []   \n",
       "511792                         []                         []   \n",
       "940525               [1, 68, 129]               [2, 69, 130]   \n",
       "\n",
       "                                                   labels  contains_ner  \n",
       "611381  [person, person, person, person, person, locat...          True  \n",
       "155556                       [person, location, location]          True  \n",
       "222753                                           [person]          True  \n",
       "99499                                                  []         False  \n",
       "275143                                                 []         False  \n",
       "710148                                   [person, person]          True  \n",
       "760664                                                 []         False  \n",
       "24406                                                  []         False  \n",
       "511792                                                 []         False  \n",
       "940525                           [person, person, person]          True  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_data.sample(10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the chunked data\n",
    "chunked_data.to_parquet(f\"anon_data/chunked_ner_data_2.parquet\",engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-empty chunks : 434666\n",
      "number of emtpy chunks : 808051\n",
      "ratio empty to non-empty : 1.859015888061178\n"
     ]
    }
   ],
   "source": [
    "n_nempty = chunked_data['contains_ner'].sum()\n",
    "n_empty = len(chunked_data)-n_nempty\n",
    "print(f\"number of non-empty chunks : {n_nempty}\")\n",
    "print(f\"number of emtpy chunks : {n_empty}\")\n",
    "print(f\"ratio empty to non-empty : {n_empty/n_nempty}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do document-based split into train, validation, test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = pd.read_parquet(f\"anon_data/chunked_ner_data_2.parquet\",engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>start_token_indices</th>\n",
       "      <th>end_token_indices</th>\n",
       "      <th>labels</th>\n",
       "      <th>contains_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611381</th>\n",
       "      <td>67938</td>\n",
       "      <td>[Partecipanti, al, procedimento, Elisabeth, Ra...</td>\n",
       "      <td>[3, 134, 209, 83, 93, 89]</td>\n",
       "      <td>[4, 135, 210, 84, 94, 89]</td>\n",
       "      <td>[person, person, person, person, person, locat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155556</th>\n",
       "      <td>18721</td>\n",
       "      <td>[Oktober, 2006, ein, Panvertebralsyndrom, bei,...</td>\n",
       "      <td>[20, 168, 70]</td>\n",
       "      <td>[21, 168, 72]</td>\n",
       "      <td>[person, location, location]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222753</th>\n",
       "      <td>26330</td>\n",
       "      <td>[Jahren, nicht, mehr, tragbar, war, und, -, na...</td>\n",
       "      <td>[115]</td>\n",
       "      <td>[116]</td>\n",
       "      <td>[person]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99499</th>\n",
       "      <td>12384</td>\n",
       "      <td>[1015, ;, PETER, HÄNNI, ,, Planungs, -, Bau, -...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275143</th>\n",
       "      <td>32021</td>\n",
       "      <td>[Sinne, von, Art, ., 98, BGG, der, Beschwerde,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710148</th>\n",
       "      <td>77708</td>\n",
       "      <td>[père, et, la, mère, bénéficiant, d, ', un, li...</td>\n",
       "      <td>[215, 98]</td>\n",
       "      <td>[216, 99]</td>\n",
       "      <td>[person, person]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760664</th>\n",
       "      <td>82462</td>\n",
       "      <td>[., Aucun, fait, nouveau, ni, preuve, nouvelle...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24406</th>\n",
       "      <td>3515</td>\n",
       "      <td>[im, Sinne, von, Art, ., 16, MWSTV, nur, dann,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511792</th>\n",
       "      <td>57662</td>\n",
       "      <td>[von, weiteren, Eingaben, und, Verfahren, in, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940525</th>\n",
       "      <td>99083</td>\n",
       "      <td>[Verfahrensbeteiligte, Tonie, Hochstrasser, ,,...</td>\n",
       "      <td>[1, 68, 129]</td>\n",
       "      <td>[2, 69, 130]</td>\n",
       "      <td>[person, person, person]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        document_id                                     tokenized_text  \\\n",
       "611381        67938  [Partecipanti, al, procedimento, Elisabeth, Ra...   \n",
       "155556        18721  [Oktober, 2006, ein, Panvertebralsyndrom, bei,...   \n",
       "222753        26330  [Jahren, nicht, mehr, tragbar, war, und, -, na...   \n",
       "99499         12384  [1015, ;, PETER, HÄNNI, ,, Planungs, -, Bau, -...   \n",
       "275143        32021  [Sinne, von, Art, ., 98, BGG, der, Beschwerde,...   \n",
       "710148        77708  [père, et, la, mère, bénéficiant, d, ', un, li...   \n",
       "760664        82462  [., Aucun, fait, nouveau, ni, preuve, nouvelle...   \n",
       "24406          3515  [im, Sinne, von, Art, ., 16, MWSTV, nur, dann,...   \n",
       "511792        57662  [von, weiteren, Eingaben, und, Verfahren, in, ...   \n",
       "940525        99083  [Verfahrensbeteiligte, Tonie, Hochstrasser, ,,...   \n",
       "\n",
       "              start_token_indices          end_token_indices  \\\n",
       "611381  [3, 134, 209, 83, 93, 89]  [4, 135, 210, 84, 94, 89]   \n",
       "155556              [20, 168, 70]              [21, 168, 72]   \n",
       "222753                      [115]                      [116]   \n",
       "99499                          []                         []   \n",
       "275143                         []                         []   \n",
       "710148                  [215, 98]                  [216, 99]   \n",
       "760664                         []                         []   \n",
       "24406                          []                         []   \n",
       "511792                         []                         []   \n",
       "940525               [1, 68, 129]               [2, 69, 130]   \n",
       "\n",
       "                                                   labels  contains_ner  \n",
       "611381  [person, person, person, person, person, locat...          True  \n",
       "155556                       [person, location, location]          True  \n",
       "222753                                           [person]          True  \n",
       "99499                                                  []         False  \n",
       "275143                                                 []         False  \n",
       "710148                                   [person, person]          True  \n",
       "760664                                                 []         False  \n",
       "24406                                                  []         False  \n",
       "511792                                                 []         False  \n",
       "940525                           [person, person, person]          True  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_data.sample(10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max document id : 124088\n",
      "number of documents : 124089\n",
      "number of training docs : 86862\n",
      "number of validation docs : 18613\n",
      "number of test docs : 18614\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "max_doc_id = chunked_data[\"document_id\"].max()\n",
    "print(f\"max document id : {max_doc_id}\")\n",
    "n_docs = max_doc_id + 1\n",
    "print(f\"number of documents : {n_docs}\")\n",
    "\n",
    "#split doc_ids into _ptrain,_pval,_ptest split\n",
    "\n",
    "shuffled_ids = list(range(n_docs))\n",
    "random.seed(42)\n",
    "random.shuffle(shuffled_ids)\n",
    "\n",
    "ids_train, ids_val, ids_test = np.split(shuffled_ids,[int(n_docs*_ptrain),int(n_docs*(_ptrain+_pval))])\n",
    "\n",
    "print(f\"number of training docs : {len(ids_train)}\")\n",
    "print(f\"number of validation docs : {len(ids_val)}\")\n",
    "print(f\"number of test docs : {len(ids_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save training data\n",
    "chunked_data[chunked_data['document_id'].isin(ids_train)].reset_index(drop=True).to_parquet(f\"anon_data/chunked_ner_data_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save validation data\n",
    "chunked_data[chunked_data['document_id'].isin(ids_val)].reset_index(drop=True).to_parquet(f\"anon_data/chunked_ner_data_val.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save validation data\n",
    "chunked_data[chunked_data['document_id'].isin(ids_test)].reset_index(drop=True).to_parquet(f\"anon_data/chunked_ner_data_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split each set into empty and non-empty samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json_non_empty(data: pd.DataFrame) -> list[dict]:\n",
    "\n",
    "    \n",
    "    ner = data.apply(lambda x: [[int(start),int(end),str(label)] for start,end,label in zip(x[\"start_token_indices\"],x[\"end_token_indices\"],x[\"labels\"])],\n",
    "                     axis=1)\n",
    "\n",
    "    return [{\"tokenized_text\" : list(text),\n",
    "             \"ner\" : list(ner)} for text,ner in zip(data['tokenized_text'],ner)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json_empty(data: pd.DataFrame,labels) -> list[dict]:\n",
    "\n",
    "\n",
    "    return [{\"tokenized_text\" : list(text),\n",
    "             \"ner\" : [],\n",
    "             \"label\" : list(labels)} for text in data['tokenized_text']] #see training with empty data : https://github.com/urchade/GLiNER/issues/139\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_json(data,split_empty = True,labels = ['person','organization','location','law','citation']):\n",
    "\n",
    "\n",
    "    if split_empty:\n",
    "        return df_to_json_empty(data[~data['contains_ner']],labels=labels), df_to_json_non_empty(data[data['contains_ner']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_train = pd.read_parquet(f\"anon_data/chunked_ner_data_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>start_token_indices</th>\n",
       "      <th>end_token_indices</th>\n",
       "      <th>labels</th>\n",
       "      <th>contains_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Parteien, Maëlle, Meyer, ,, Beschwerdeführer,...</td>\n",
       "      <td>[1, 69, 131, 175]</td>\n",
       "      <td>[2, 70, 132, 176]</td>\n",
       "      <td>[person, person, person, person]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[dem, Antrag, des, Untersuchungsrichters, -, d...</td>\n",
       "      <td>[53, 224, 249]</td>\n",
       "      <td>[54, 225, 250]</td>\n",
       "      <td>[person, person, person]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[Verteidigung, und, hält, in, seiner, Replik, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Dazu, ist, er, befugt, ., Die, Nichtigkeit, e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[schriftlicher, Einwilligung, der, inhaftierte...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                     tokenized_text  \\\n",
       "0            0  [Parteien, Maëlle, Meyer, ,, Beschwerdeführer,...   \n",
       "1            0  [dem, Antrag, des, Untersuchungsrichters, -, d...   \n",
       "2            0  [Verteidigung, und, hält, in, seiner, Replik, ...   \n",
       "3            0  [Dazu, ist, er, befugt, ., Die, Nichtigkeit, e...   \n",
       "4            0  [schriftlicher, Einwilligung, der, inhaftierte...   \n",
       "\n",
       "  start_token_indices  end_token_indices                            labels  \\\n",
       "0   [1, 69, 131, 175]  [2, 70, 132, 176]  [person, person, person, person]   \n",
       "1      [53, 224, 249]     [54, 225, 250]          [person, person, person]   \n",
       "2                  []                 []                                []   \n",
       "3                  []                 []                                []   \n",
       "4                  []                 []                                []   \n",
       "\n",
       "   contains_ner  \n",
       "0          True  \n",
       "1          True  \n",
       "2         False  \n",
       "3         False  \n",
       "4         False  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(chunked_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_empty,train_json_non_empty = df_to_json(chunked_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303458\n",
      "303458\n",
      "564961\n",
      "564961\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(chunked_train[chunked_train['contains_ner']]))\n",
    "print(len(train_json_non_empty))\n",
    "print(len(chunked_train[~chunked_train['contains_ner']]))\n",
    "print(len(train_json_empty))\n",
    "\n",
    "print(n_train == len(train_json_empty) + len(train_json_non_empty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data\n",
    "\n",
    "with open(f\"anon_data/train/train_non_empty.json\",'w') as f:\n",
    "    json.dump(train_json_non_empty,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data\n",
    "with open(f\"anon_data/train/train_empty.json\",'w') as f:\n",
    "    json.dump(train_json_empty,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear memory\n",
    "del chunked_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_json_non_empty,train_json_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_val = pd.read_parquet(f\"anon_data/chunked_ner_data_val.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_json_empty,val_json_non_empty = df_to_json(chunked_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data\n",
    "with open(f\"anon_data/train/validation_empty.json\",'w') as f:\n",
    "    json.dump(val_json_empty,f)\n",
    "\n",
    "#Save data\n",
    "with open(f\"anon_data/train/validation_non_empty.json\",'w') as f:\n",
    "    json.dump(val_json_non_empty,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release memory\n",
    "del chunked_val,val_json_empty,val_json_non_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_test = pd.read_parquet(f\"anon_data/chunked_ner_data_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json_empty,test_json_non_empty = df_to_json(chunked_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data\n",
    "with open(f\"anon_data/test/test_empty.json\",'w') as f:\n",
    "    json.dump(test_json_empty,f)\n",
    "\n",
    "#Save data\n",
    "with open(f\"anon_data/test/test_non_empty.json\",'w') as f:\n",
    "    json.dump(test_json_non_empty,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSL_NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
